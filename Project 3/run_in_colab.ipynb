{"cells":[{"cell_type":"code","source":["from google.colab import drive\n","drive.mount('/content/drive')"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"692matSiWpuQ","executionInfo":{"status":"ok","timestamp":1742082640595,"user_tz":240,"elapsed":853,"user":{"displayName":"Xu Jessica","userId":"02326492230102720065"}},"outputId":"53fc60c4-2e43-4690-d7fd-7bf8ae7410d6"},"execution_count":4,"outputs":[{"output_type":"stream","name":"stdout","text":["Drive already mounted at /content/drive; to attempt to forcibly remount, call drive.mount(\"/content/drive\", force_remount=True).\n"]}]},{"cell_type":"code","source":["%cd /content/drive/MyDrive/handout"],"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"uvz96FuNW6HQ","executionInfo":{"status":"ok","timestamp":1742082651326,"user_tz":240,"elapsed":50,"user":{"displayName":"Xu Jessica","userId":"02326492230102720065"}},"outputId":"628bda80-12bb-40bb-d596-ca7d17b26c24"},"execution_count":5,"outputs":[{"output_type":"stream","name":"stdout","text":["/content/drive/MyDrive/handout\n"]}]},{"cell_type":"code","execution_count":6,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"nw-TS7ySYbEd","outputId":"898dab7b-851b-4c30-8950-f23a23f3a31e","executionInfo":{"status":"ok","timestamp":1742082756349,"user_tz":240,"elapsed":100872,"user":{"displayName":"Xu Jessica","userId":"02326492230102720065"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["Requirement already satisfied: torch in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 1)) (2.6.0+cu124)\n","Requirement already satisfied: transformers in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 2)) (4.48.3)\n","Collecting datasets (from -r requirements.txt (line 3))\n","  Downloading datasets-3.4.0-py3-none-any.whl.metadata (19 kB)\n","Collecting tiktoken (from -r requirements.txt (line 4))\n","  Downloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.7 kB)\n","Requirement already satisfied: wandb in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 5)) (0.19.8)\n","Requirement already satisfied: tqdm in /usr/local/lib/python3.11/dist-packages (from -r requirements.txt (line 6)) (4.67.1)\n","Requirement already satisfied: filelock in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 1)) (3.17.0)\n","Requirement already satisfied: typing-extensions>=4.10.0 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 1)) (4.12.2)\n","Requirement already satisfied: networkx in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 1)) (3.4.2)\n","Requirement already satisfied: jinja2 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 1)) (3.1.6)\n","Requirement already satisfied: fsspec in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 1)) (2024.10.0)\n","Collecting nvidia-cuda-nvrtc-cu12==12.4.127 (from torch->-r requirements.txt (line 1))\n","  Downloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-runtime-cu12==12.4.127 (from torch->-r requirements.txt (line 1))\n","  Downloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cuda-cupti-cu12==12.4.127 (from torch->-r requirements.txt (line 1))\n","  Downloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cudnn-cu12==9.1.0.70 (from torch->-r requirements.txt (line 1))\n","  Downloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cublas-cu12==12.4.5.8 (from torch->-r requirements.txt (line 1))\n","  Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cufft-cu12==11.2.1.3 (from torch->-r requirements.txt (line 1))\n","  Downloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-curand-cu12==10.3.5.147 (from torch->-r requirements.txt (line 1))\n","  Downloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Collecting nvidia-cusolver-cu12==11.6.1.9 (from torch->-r requirements.txt (line 1))\n","  Downloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Collecting nvidia-cusparse-cu12==12.3.1.170 (from torch->-r requirements.txt (line 1))\n","  Downloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl.metadata (1.6 kB)\n","Requirement already satisfied: nvidia-cusparselt-cu12==0.6.2 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 1)) (0.6.2)\n","Requirement already satisfied: nvidia-nccl-cu12==2.21.5 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 1)) (2.21.5)\n","Requirement already satisfied: nvidia-nvtx-cu12==12.4.127 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 1)) (12.4.127)\n","Collecting nvidia-nvjitlink-cu12==12.4.127 (from torch->-r requirements.txt (line 1))\n","  Downloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl.metadata (1.5 kB)\n","Requirement already satisfied: triton==3.2.0 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 1)) (3.2.0)\n","Requirement already satisfied: sympy==1.13.1 in /usr/local/lib/python3.11/dist-packages (from torch->-r requirements.txt (line 1)) (1.13.1)\n","Requirement already satisfied: mpmath<1.4,>=1.1.0 in /usr/local/lib/python3.11/dist-packages (from sympy==1.13.1->torch->-r requirements.txt (line 1)) (1.3.0)\n","Requirement already satisfied: huggingface-hub<1.0,>=0.24.0 in /usr/local/lib/python3.11/dist-packages (from transformers->-r requirements.txt (line 2)) (0.28.1)\n","Requirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from transformers->-r requirements.txt (line 2)) (1.26.4)\n","Requirement already satisfied: packaging>=20.0 in /usr/local/lib/python3.11/dist-packages (from transformers->-r requirements.txt (line 2)) (24.2)\n","Requirement already satisfied: pyyaml>=5.1 in /usr/local/lib/python3.11/dist-packages (from transformers->-r requirements.txt (line 2)) (6.0.2)\n","Requirement already satisfied: regex!=2019.12.17 in /usr/local/lib/python3.11/dist-packages (from transformers->-r requirements.txt (line 2)) (2024.11.6)\n","Requirement already satisfied: requests in /usr/local/lib/python3.11/dist-packages (from transformers->-r requirements.txt (line 2)) (2.32.3)\n","Requirement already satisfied: tokenizers<0.22,>=0.21 in /usr/local/lib/python3.11/dist-packages (from transformers->-r requirements.txt (line 2)) (0.21.0)\n","Requirement already satisfied: safetensors>=0.4.1 in /usr/local/lib/python3.11/dist-packages (from transformers->-r requirements.txt (line 2)) (0.5.3)\n","Requirement already satisfied: pyarrow>=15.0.0 in /usr/local/lib/python3.11/dist-packages (from datasets->-r requirements.txt (line 3)) (18.1.0)\n","Collecting dill<0.3.9,>=0.3.0 (from datasets->-r requirements.txt (line 3))\n","  Downloading dill-0.3.8-py3-none-any.whl.metadata (10 kB)\n","Requirement already satisfied: pandas in /usr/local/lib/python3.11/dist-packages (from datasets->-r requirements.txt (line 3)) (2.2.2)\n","Collecting xxhash (from datasets->-r requirements.txt (line 3))\n","  Downloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (12 kB)\n","Collecting multiprocess<0.70.17 (from datasets->-r requirements.txt (line 3))\n","  Downloading multiprocess-0.70.16-py311-none-any.whl.metadata (7.2 kB)\n","Requirement already satisfied: aiohttp in /usr/local/lib/python3.11/dist-packages (from datasets->-r requirements.txt (line 3)) (3.11.13)\n","Requirement already satisfied: click!=8.0.0,>=7.1 in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 5)) (8.1.8)\n","Requirement already satisfied: docker-pycreds>=0.4.0 in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 5)) (0.4.0)\n","Requirement already satisfied: gitpython!=3.1.29,>=1.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 5)) (3.1.44)\n","Requirement already satisfied: platformdirs in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 5)) (4.3.6)\n","Requirement already satisfied: protobuf!=4.21.0,!=5.28.0,<6,>=3.19.0 in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 5)) (4.25.6)\n","Requirement already satisfied: psutil>=5.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 5)) (5.9.5)\n","Requirement already satisfied: pydantic<3,>=2.6 in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 5)) (2.10.6)\n","Requirement already satisfied: sentry-sdk>=2.0.0 in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 5)) (2.22.0)\n","Requirement already satisfied: setproctitle in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 5)) (1.3.5)\n","Requirement already satisfied: setuptools in /usr/local/lib/python3.11/dist-packages (from wandb->-r requirements.txt (line 5)) (75.1.0)\n","Requirement already satisfied: six>=1.4.0 in /usr/local/lib/python3.11/dist-packages (from docker-pycreds>=0.4.0->wandb->-r requirements.txt (line 5)) (1.17.0)\n","Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (2.6.1)\n","Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (1.3.2)\n","Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (25.2.0)\n","Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (1.5.0)\n","Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (6.1.0)\n","Requirement already satisfied: propcache>=0.2.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (0.3.0)\n","Requirement already satisfied: yarl<2.0,>=1.17.0 in /usr/local/lib/python3.11/dist-packages (from aiohttp->datasets->-r requirements.txt (line 3)) (1.18.3)\n","Requirement already satisfied: gitdb<5,>=4.0.1 in /usr/local/lib/python3.11/dist-packages (from gitpython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 5)) (4.0.12)\n","Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb->-r requirements.txt (line 5)) (0.7.0)\n","Requirement already satisfied: pydantic-core==2.27.2 in /usr/local/lib/python3.11/dist-packages (from pydantic<3,>=2.6->wandb->-r requirements.txt (line 5)) (2.27.2)\n","Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->-r requirements.txt (line 2)) (3.4.1)\n","Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->-r requirements.txt (line 2)) (3.10)\n","Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->-r requirements.txt (line 2)) (2.3.0)\n","Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.11/dist-packages (from requests->transformers->-r requirements.txt (line 2)) (2025.1.31)\n","Requirement already satisfied: MarkupSafe>=2.0 in /usr/local/lib/python3.11/dist-packages (from jinja2->torch->-r requirements.txt (line 1)) (3.0.2)\n","Requirement already satisfied: python-dateutil>=2.8.2 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->-r requirements.txt (line 3)) (2.8.2)\n","Requirement already satisfied: pytz>=2020.1 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->-r requirements.txt (line 3)) (2025.1)\n","Requirement already satisfied: tzdata>=2022.7 in /usr/local/lib/python3.11/dist-packages (from pandas->datasets->-r requirements.txt (line 3)) (2025.1)\n","Requirement already satisfied: smmap<6,>=3.0.1 in /usr/local/lib/python3.11/dist-packages (from gitdb<5,>=4.0.1->gitpython!=3.1.29,>=1.0.0->wandb->-r requirements.txt (line 5)) (5.0.2)\n","Downloading nvidia_cublas_cu12-12.4.5.8-py3-none-manylinux2014_x86_64.whl (363.4 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m363.4/363.4 MB\u001b[0m \u001b[31m1.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_cupti_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (13.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m13.8/13.8 MB\u001b[0m \u001b[31m104.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_nvrtc_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (24.6 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m24.6/24.6 MB\u001b[0m \u001b[31m84.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cuda_runtime_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (883 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m883.7/883.7 kB\u001b[0m \u001b[31m57.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cudnn_cu12-9.1.0.70-py3-none-manylinux2014_x86_64.whl (664.8 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m664.8/664.8 MB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cufft_cu12-11.2.1.3-py3-none-manylinux2014_x86_64.whl (211.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m211.5/211.5 MB\u001b[0m \u001b[31m5.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_curand_cu12-10.3.5.147-py3-none-manylinux2014_x86_64.whl (56.3 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m56.3/56.3 MB\u001b[0m \u001b[31m11.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusolver_cu12-11.6.1.9-py3-none-manylinux2014_x86_64.whl (127.9 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m127.9/127.9 MB\u001b[0m \u001b[31m7.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_cusparse_cu12-12.3.1.170-py3-none-manylinux2014_x86_64.whl (207.5 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m207.5/207.5 MB\u001b[0m \u001b[31m5.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading nvidia_nvjitlink_cu12-12.4.127-py3-none-manylinux2014_x86_64.whl (21.1 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m21.1/21.1 MB\u001b[0m \u001b[31m89.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading datasets-3.4.0-py3-none-any.whl (487 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m487.4/487.4 kB\u001b[0m \u001b[31m38.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading tiktoken-0.9.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m72.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading dill-0.3.8-py3-none-any.whl (116 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m116.3/116.3 kB\u001b[0m \u001b[31m12.2 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading multiprocess-0.70.16-py311-none-any.whl (143 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m143.5/143.5 kB\u001b[0m \u001b[31m15.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hDownloading xxhash-3.5.0-cp311-cp311-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (194 kB)\n","\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m194.8/194.8 kB\u001b[0m \u001b[31m19.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n","\u001b[?25hInstalling collected packages: xxhash, nvidia-nvjitlink-cu12, nvidia-curand-cu12, nvidia-cufft-cu12, nvidia-cuda-runtime-cu12, nvidia-cuda-nvrtc-cu12, nvidia-cuda-cupti-cu12, nvidia-cublas-cu12, dill, tiktoken, nvidia-cusparse-cu12, nvidia-cudnn-cu12, multiprocess, nvidia-cusolver-cu12, datasets\n","  Attempting uninstall: nvidia-nvjitlink-cu12\n","    Found existing installation: nvidia-nvjitlink-cu12 12.5.82\n","    Uninstalling nvidia-nvjitlink-cu12-12.5.82:\n","      Successfully uninstalled nvidia-nvjitlink-cu12-12.5.82\n","  Attempting uninstall: nvidia-curand-cu12\n","    Found existing installation: nvidia-curand-cu12 10.3.6.82\n","    Uninstalling nvidia-curand-cu12-10.3.6.82:\n","      Successfully uninstalled nvidia-curand-cu12-10.3.6.82\n","  Attempting uninstall: nvidia-cufft-cu12\n","    Found existing installation: nvidia-cufft-cu12 11.2.3.61\n","    Uninstalling nvidia-cufft-cu12-11.2.3.61:\n","      Successfully uninstalled nvidia-cufft-cu12-11.2.3.61\n","  Attempting uninstall: nvidia-cuda-runtime-cu12\n","    Found existing installation: nvidia-cuda-runtime-cu12 12.5.82\n","    Uninstalling nvidia-cuda-runtime-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-runtime-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-nvrtc-cu12\n","    Found existing installation: nvidia-cuda-nvrtc-cu12 12.5.82\n","    Uninstalling nvidia-cuda-nvrtc-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-nvrtc-cu12-12.5.82\n","  Attempting uninstall: nvidia-cuda-cupti-cu12\n","    Found existing installation: nvidia-cuda-cupti-cu12 12.5.82\n","    Uninstalling nvidia-cuda-cupti-cu12-12.5.82:\n","      Successfully uninstalled nvidia-cuda-cupti-cu12-12.5.82\n","  Attempting uninstall: nvidia-cublas-cu12\n","    Found existing installation: nvidia-cublas-cu12 12.5.3.2\n","    Uninstalling nvidia-cublas-cu12-12.5.3.2:\n","      Successfully uninstalled nvidia-cublas-cu12-12.5.3.2\n","  Attempting uninstall: nvidia-cusparse-cu12\n","    Found existing installation: nvidia-cusparse-cu12 12.5.1.3\n","    Uninstalling nvidia-cusparse-cu12-12.5.1.3:\n","      Successfully uninstalled nvidia-cusparse-cu12-12.5.1.3\n","  Attempting uninstall: nvidia-cudnn-cu12\n","    Found existing installation: nvidia-cudnn-cu12 9.3.0.75\n","    Uninstalling nvidia-cudnn-cu12-9.3.0.75:\n","      Successfully uninstalled nvidia-cudnn-cu12-9.3.0.75\n","  Attempting uninstall: nvidia-cusolver-cu12\n","    Found existing installation: nvidia-cusolver-cu12 11.6.3.83\n","    Uninstalling nvidia-cusolver-cu12-11.6.3.83:\n","      Successfully uninstalled nvidia-cusolver-cu12-11.6.3.83\n","Successfully installed datasets-3.4.0 dill-0.3.8 multiprocess-0.70.16 nvidia-cublas-cu12-12.4.5.8 nvidia-cuda-cupti-cu12-12.4.127 nvidia-cuda-nvrtc-cu12-12.4.127 nvidia-cuda-runtime-cu12-12.4.127 nvidia-cudnn-cu12-9.1.0.70 nvidia-cufft-cu12-11.2.1.3 nvidia-curand-cu12-10.3.5.147 nvidia-cusolver-cu12-11.6.1.9 nvidia-cusparse-cu12-12.3.1.170 nvidia-nvjitlink-cu12-12.4.127 tiktoken-0.9.0 xxhash-3.5.0\n"]}],"source":["# Install the required packages\n","!pip install -r requirements.txt"]},{"cell_type":"markdown","metadata":{"id":"4i7qmxIFQw6M"},"source":["# Check results without any finetuning"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"_aKpWCzHLHwg","outputId":"a9c62924-e3b5-4296-8ef0-92bf27c8c22e","executionInfo":{"status":"ok","timestamp":1742051943537,"user_tz":240,"elapsed":353369,"user":{"displayName":"Xu Jessica","userId":"02326492230102720065"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["2025-03-15 15:13:23.918708: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1742051604.251080    2001 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1742051604.336172    2001 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2025-03-15 15:13:25.024672: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","Generating train split: 100% 8530/8530 [00:00<00:00, 14537.79 examples/s]\n","Generating validation split: 100% 1066/1066 [00:00<00:00, 148389.63 examples/s]\n","Generating test split: 100% 1066/1066 [00:00<00:00, 167828.84 examples/s]\n","loading weights from pretrained gpt: gpt2-medium\n","forcing vocab_size=50257, block_size=1024, bias=True\n","overriding dropout rate to 0.0\n","number of parameters: 353.77M\n","config.json: 100% 718/718 [00:00<00:00, 4.10MB/s]\n","model.safetensors: 100% 1.52G/1.52G [00:13<00:00, 111MB/s]\n","generation_config.json: 100% 124/124 [00:00<00:00, 820kB/s]\n","100% 1066/1066 [02:05<00:00,  8.52it/s]\n","Best Val Checkpoint || Accuracy: 0.1050656660412758, Positive Predictions: 181, Negative Predictions: 0, Correct Predictions: 112\n","loading weights from pretrained gpt: gpt2-medium\n","forcing vocab_size=50257, block_size=1024, bias=True\n","overriding dropout rate to 0.0\n","number of parameters: 353.77M\n","100% 1066/1066 [02:04<00:00,  8.59it/s]\n","Last Iter Checkpoint || Accuracy: 0.1050656660412758, Positive Predictions: 181, Negative Predictions: 0, Correct Predictions: 112\n"]}],"source":["!python generate.py --init_from=\"gpt2-medium\""]},{"cell_type":"markdown","metadata":{"id":"4cGeSYLsQw6M"},"source":["# LoRA with rank = 16, alpha = 64"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"73Ydq2uufIJw","outputId":"f0b079b3-d2fd-45d1-a9d4-c684d1f3df90","executionInfo":{"status":"ok","timestamp":1742053672730,"user_tz":240,"elapsed":1554765,"user":{"displayName":"Xu Jessica","userId":"02326492230102720065"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["2025-03-15 15:22:09.784120: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1742052129.810811    4223 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1742052129.822961    4223 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2025-03-15 15:22:09.854482: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n","\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n","\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n","\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n","\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 2\n","\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n","\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n","\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mxulixin\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.8\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/MyDrive/handout/wandb/run-20250315_152415-97c38yl8\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mgpt-r16_a64-lora-r:16-alph:64-lr:0.00025-iter:80\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/xulixin/HW3_lora_finetune_handout\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/xulixin/HW3_lora_finetune_handout/runs/97c38yl8\u001b[0m\n","ddp False\n","tokens per iteration will be: 131,072\n","tokenizer_config.json: 100% 26.0/26.0 [00:00<00:00, 58.1kB/s]\n","config.json: 100% 665/665 [00:00<00:00, 1.33MB/s]\n","vocab.json: 100% 1.04M/1.04M [00:00<00:00, 2.44MB/s]\n","merges.txt: 100% 456k/456k [00:00<00:00, 22.5MB/s]\n","tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 1.58MB/s]\n","Map: 100% 5000/5000 [00:02<00:00, 1778.55 examples/s]\n","Map: 100% 1066/1066 [00:00<00:00, 2045.33 examples/s]\n","Positive: 2478, Negative: 2522\n","loading weights from pretrained gpt: gpt2-medium\n","forcing vocab_size=50257, block_size=1024, bias=True\n","overriding dropout rate to 0.0\n","overriding lora_rank and lora_alpha to 16, 64\n","overriding lora_dropout to 0.05\n","number of parameters: 356.13M\n","/content/drive/MyDrive/handout/train.py:237: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler(enabled=(args.dtype == 'float16'))\n","num decayed parameter tensors: 96, with 2,359,296 parameters\n","num non-decayed parameter tensors: 0, with 0 parameters\n","using fused AdamW: True\n","iter 1: loss 9.2426, time 10417.83ms, mfu -100.00%\n","iter 2: loss 8.4222, time 9577.41ms, mfu -100.00%\n","iter 3: loss 6.8660, time 9775.46ms, mfu -100.00%\n","iter 4: loss 6.1105, time 9734.57ms, mfu -100.00%\n","Evaluation at Iter 5: Val Loss 5.8937\n","Best val loss so far, saving Best Val checkpoint...\n","iter 5: loss 6.2148, time 21935.94ms, mfu -100.00%\n","iter 6: loss 3.5170, time 9934.86ms, mfu 10.31%\n","iter 7: loss 2.6392, time 10178.66ms, mfu 10.29%\n","iter 8: loss 3.4894, time 10450.54ms, mfu 10.24%\n","iter 9: loss 2.1542, time 10004.73ms, mfu 10.24%\n","Evaluation at Iter 10: Val Loss 1.1423\n","Best val loss so far, saving Best Val checkpoint...\n","iter 10: loss 1.6059, time 19865.21ms, mfu 9.73%\n","iter 11: loss 0.6930, time 10608.07ms, mfu 9.72%\n","iter 12: loss 0.6855, time 10401.34ms, mfu 9.74%\n","iter 13: loss 0.2228, time 10841.93ms, mfu 9.71%\n","iter 14: loss 0.1066, time 10381.94ms, mfu 9.72%\n","Evaluation at Iter 15: Val Loss 0.0698\n","Best val loss so far, saving Best Val checkpoint...\n","iter 15: loss 0.0247, time 19555.78ms, mfu 9.28%\n","iter 16: loss 0.0507, time 10209.39ms, mfu 9.35%\n","iter 17: loss 0.0599, time 10548.32ms, mfu 9.39%\n","iter 18: loss 0.0510, time 10656.27ms, mfu 9.41%\n","iter 19: loss 0.0237, time 10713.84ms, mfu 9.43%\n","Evaluation at Iter 20: Val Loss 0.0451\n","Best val loss so far, saving Best Val checkpoint...\n","iter 20: loss 0.0179, time 20630.58ms, mfu 8.98%\n","iter 21: loss 0.0343, time 10514.60ms, mfu 9.06%\n","iter 22: loss 0.0199, time 10568.36ms, mfu 9.12%\n","iter 23: loss 0.0674, time 10456.35ms, mfu 9.19%\n","iter 24: loss 0.0535, time 10222.06ms, mfu 9.27%\n","Evaluation at Iter 25: Val Loss 0.0238\n","Best val loss so far, saving Best Val checkpoint...\n","iter 25: loss 0.1304, time 20247.61ms, mfu 8.85%\n","iter 26: loss 0.0063, time 10555.45ms, mfu 8.94%\n","iter 27: loss 0.0063, time 10680.25ms, mfu 9.00%\n","iter 28: loss 0.0082, time 10919.42ms, mfu 9.04%\n","iter 29: loss 0.0368, time 10312.53ms, mfu 9.13%\n","Evaluation at Iter 30: Val Loss 0.0190\n","Best val loss so far, saving Best Val checkpoint...\n","iter 30: loss 0.0061, time 21435.42ms, mfu 8.69%\n","iter 31: loss 0.0114, time 10595.95ms, mfu 8.79%\n","iter 32: loss 0.0386, time 10552.44ms, mfu 8.88%\n","iter 33: loss 0.0533, time 10851.66ms, mfu 8.94%\n","iter 34: loss 0.0393, time 10394.57ms, mfu 9.03%\n","Evaluation at Iter 35: Val Loss 0.0273\n","iter 35: loss 0.0412, time 13488.98ms, mfu 8.89%\n","iter 36: loss 0.0311, time 10346.38ms, mfu 8.99%\n","iter 37: loss 0.0085, time 10328.22ms, mfu 9.08%\n","iter 38: loss 0.0531, time 10678.85ms, mfu 9.13%\n","iter 39: loss 0.0085, time 10405.14ms, mfu 9.20%\n","Evaluation at Iter 40: Val Loss 0.0374\n","iter 40: loss 0.0063, time 13543.00ms, mfu 9.04%\n","iter 41: loss 0.0364, time 10537.08ms, mfu 9.11%\n","iter 42: loss 0.0056, time 10639.98ms, mfu 9.16%\n","iter 43: loss 0.0376, time 10507.31ms, mfu 9.22%\n","iter 44: loss 0.0043, time 10494.70ms, mfu 9.27%\n","Evaluation at Iter 45: Val Loss 0.0326\n","iter 45: loss 0.0024, time 13918.42ms, mfu 9.08%\n","iter 46: loss 0.0621, time 10316.32ms, mfu 9.17%\n","iter 47: loss 0.0059, time 10608.45ms, mfu 9.22%\n","iter 48: loss 0.0049, time 10119.22ms, mfu 9.31%\n","iter 49: loss 0.0053, time 9954.37ms, mfu 9.41%\n","Evaluation at Iter 50: Val Loss 0.0294\n","iter 50: loss 0.0049, time 13549.66ms, mfu 9.22%\n","iter 51: loss 0.0092, time 10552.24ms, mfu 9.27%\n","iter 52: loss 0.0809, time 10415.39ms, mfu 9.33%\n","iter 53: loss 0.0680, time 10636.04ms, mfu 9.36%\n","iter 54: loss 0.0280, time 10325.36ms, mfu 9.41%\n","Evaluation at Iter 55: Val Loss 0.0258\n","iter 55: loss 0.0099, time 13701.64ms, mfu 9.22%\n","iter 56: loss 0.0039, time 10625.49ms, mfu 9.26%\n","iter 57: loss 0.0291, time 10536.22ms, mfu 9.31%\n","iter 58: loss 0.0680, time 10311.66ms, mfu 9.37%\n","iter 59: loss 0.0032, time 10661.12ms, mfu 9.40%\n","Evaluation at Iter 60: Val Loss 0.0265\n","iter 60: loss 0.0066, time 13566.18ms, mfu 9.21%\n","iter 61: loss 0.0403, time 10502.90ms, mfu 9.27%\n","iter 62: loss 0.0166, time 10234.57ms, mfu 9.34%\n","iter 63: loss 0.0016, time 10441.74ms, mfu 9.39%\n","iter 64: loss 0.0527, time 10346.21ms, mfu 9.44%\n","Evaluation at Iter 65: Val Loss 0.0227\n","iter 65: loss 0.0045, time 13478.30ms, mfu 9.25%\n","iter 66: loss 0.0041, time 10314.45ms, mfu 9.32%\n","iter 67: loss 0.0046, time 10472.73ms, mfu 9.37%\n","iter 68: loss 0.0223, time 10477.29ms, mfu 9.41%\n","iter 69: loss 0.0076, time 10356.26ms, mfu 9.46%\n","Evaluation at Iter 70: Val Loss 0.0183\n","Best val loss so far, saving Best Val checkpoint...\n","iter 70: loss 0.0049, time 27126.93ms, mfu 8.89%\n","iter 71: loss 0.0021, time 10845.84ms, mfu 8.95%\n","iter 72: loss 0.0846, time 10695.14ms, mfu 9.01%\n","iter 73: loss 0.0280, time 11027.38ms, mfu 9.04%\n","iter 74: loss 0.0050, time 10498.21ms, mfu 9.11%\n","Evaluation at Iter 75: Val Loss 0.0256\n","iter 75: loss 0.0148, time 13186.36ms, mfu 8.98%\n","iter 76: loss 0.0361, time 10489.13ms, mfu 9.05%\n","iter 77: loss 0.0277, time 10273.16ms, mfu 9.15%\n","iter 78: loss 0.0028, time 10247.56ms, mfu 9.23%\n","iter 79: loss 0.0236, time 10480.25ms, mfu 9.29%\n","Evaluation at Iter 80: Val Loss 0.0207\n","iter 80: loss 0.0139, time 13342.81ms, mfu 9.13%\n","number of parameters: 356.13M\n","100% 1066/1066 [03:13<00:00,  5.52it/s]\n","Best Val Checkpoint || Accuracy: 0.8405253283302064, Positive Predictions: 388, Negative Predictions: 677, Correct Predictions: 896\n","number of parameters: 356.13M\n","100% 1066/1066 [03:06<00:00,  5.72it/s]\n","Last Iter Checkpoint || Accuracy: 0.8424015009380863, Positive Predictions: 445, Negative Predictions: 596, Correct Predictions: 898\n","\u001b[1;34mwandb\u001b[0m: \n","\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mgpt-r16_a64-lora-r:16-alph:64-lr:0.00025-iter:80\u001b[0m at: \u001b[34mhttps://wandb.ai/xulixin/HW3_lora_finetune_handout/runs/97c38yl8\u001b[0m\n","\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250315_152415-97c38yl8/logs\u001b[0m\n"]}],"source":["!python train.py --init_from=\"gpt2-medium\" --out_dir=\"gpt-r16_a64-lora\" --lora_rank=16 --lora_alpha=64"]},{"cell_type":"markdown","metadata":{"id":"T1LPI2bSQw6M"},"source":["# Default LoRA run with rank = 128, alpha = 512"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"OndAfEQSbctC","outputId":"aa030ce2-ce89-4394-c80c-271fadfe4e9d"},"outputs":[{"output_type":"stream","name":"stdout","text":["2025-03-16 01:24:26.498920: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1742088266.715704   28472 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1742088266.783561   28472 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2025-03-16 01:24:27.281109: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mxulixin\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.8\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/MyDrive/handout/wandb/run-20250316_012433-ovxaxgbk\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mgpt-default-lora-r:128-alph:512-lr:0.00025-iter:80\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/xulixin/HW3_lora_finetune_handout\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/xulixin/HW3_lora_finetune_handout/runs/ovxaxgbk\u001b[0m\n","ddp False\n","tokens per iteration will be: 131,072\n","Map: 100% 5000/5000 [00:04<00:00, 1069.51 examples/s]\n","Map: 100% 1066/1066 [00:00<00:00, 2153.92 examples/s]\n","Positive: 2478, Negative: 2522\n","loading weights from pretrained gpt: gpt2-medium\n","forcing vocab_size=50257, block_size=1024, bias=True\n","overriding dropout rate to 0.0\n","overriding lora_rank and lora_alpha to 128, 512\n","overriding lora_dropout to 0.05\n","number of parameters: 372.65M\n","/content/drive/MyDrive/handout/train.py:237: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler(enabled=(args.dtype == 'float16'))\n","num decayed parameter tensors: 96, with 18,874,368 parameters\n","num non-decayed parameter tensors: 0, with 0 parameters\n","using fused AdamW: True\n","iter 1: loss 8.2224, time 10496.84ms, mfu -100.00%\n","iter 2: loss 5.3627, time 9268.02ms, mfu -100.00%\n","iter 3: loss 1.3818, time 9030.43ms, mfu -100.00%\n","iter 4: loss 0.2728, time 9347.57ms, mfu -100.00%\n","Evaluation at Iter 5: Val Loss 0.0946\n","Best val loss so far, saving Best Val checkpoint...\n","iter 5: loss 0.1243, time 61702.82ms, mfu -100.00%\n","iter 6: loss 0.0173, time 9143.17ms, mfu 11.66%\n","iter 7: loss 0.2781, time 9629.73ms, mfu 11.60%\n","iter 8: loss 0.0606, time 9954.63ms, mfu 11.51%\n","iter 9: loss 0.0630, time 9545.84ms, mfu 11.48%\n","Evaluation at Iter 10: Val Loss 0.0718\n","Best val loss so far, saving Best Val checkpoint...\n","iter 10: loss 0.0672, time 22205.63ms, mfu 10.81%\n","iter 11: loss 0.0569, time 9430.17ms, mfu 10.86%\n","iter 12: loss 0.0258, time 9477.87ms, mfu 10.90%\n","iter 13: loss 0.0153, time 9636.30ms, mfu 10.92%\n","iter 14: loss 0.0400, time 9515.22ms, mfu 10.94%\n","Evaluation at Iter 15: Val Loss 0.0379\n","Best val loss so far, saving Best Val checkpoint...\n","iter 15: loss 0.0192, time 24090.81ms, mfu 10.29%\n","iter 16: loss 0.0533, time 9552.02ms, mfu 10.38%\n","iter 17: loss 0.0089, time 9977.38ms, mfu 10.41%\n","iter 18: loss 0.0532, time 9555.79ms, mfu 10.48%\n","iter 19: loss 0.0084, time 9543.78ms, mfu 10.55%\n","Evaluation at Iter 20: Val Loss 0.0277\n","Best val loss so far, saving Best Val checkpoint...\n","iter 20: loss 0.0792, time 20826.74ms, mfu 10.01%\n","iter 21: loss 0.0388, time 9363.20ms, mfu 10.15%\n","iter 22: loss 0.0259, time 9718.22ms, mfu 10.23%\n","iter 23: loss 0.0068, time 9763.55ms, mfu 10.30%\n","iter 24: loss 0.0089, time 9464.26ms, mfu 10.40%\n","Evaluation at Iter 25: Val Loss 0.0337\n","iter 25: loss 0.0518, time 12653.50ms, mfu 10.20%\n","iter 26: loss 0.0309, time 9443.52ms, mfu 10.31%\n","iter 27: loss 0.0681, time 9631.54ms, mfu 10.38%\n","iter 28: loss 0.0048, time 9451.22ms, mfu 10.47%\n","iter 29: loss 0.0500, time 9350.21ms, mfu 10.57%\n","Evaluation at Iter 30: Val Loss 0.0223\n","Best val loss so far, saving Best Val checkpoint...\n","iter 30: loss 0.0548, time 23412.08ms, mfu 9.97%\n","iter 31: loss 0.0079, time 9493.39ms, mfu 10.09%\n","iter 32: loss 0.0376, time 9950.39ms, mfu 10.15%\n","iter 33: loss 0.0161, time 9956.55ms, mfu 10.21%\n","iter 34: loss 0.0482, time 9819.82ms, mfu 10.27%\n","Evaluation at Iter 35: Val Loss 0.0325\n","iter 35: loss 0.0050, time 12365.19ms, mfu 10.11%\n","iter 36: loss 0.0270, time 9269.07ms, mfu 10.25%\n","iter 37: loss 0.0107, time 9450.11ms, mfu 10.35%\n","iter 38: loss 0.0119, time 9473.81ms, mfu 10.44%\n","iter 39: loss 0.0051, time 9452.07ms, mfu 10.53%\n","Evaluation at Iter 40: Val Loss 0.0206\n","Best val loss so far, saving Best Val checkpoint...\n","iter 40: loss 0.0281, time 20838.48ms, mfu 9.98%\n","iter 41: loss 0.0117, time 9515.17ms, mfu 10.11%\n","iter 42: loss 0.0105, time 9947.92ms, mfu 10.17%\n","iter 43: loss 0.0028, time 9646.95ms, mfu 10.26%\n","iter 44: loss 0.0014, time 9646.32ms, mfu 10.34%\n","Evaluation at Iter 45: Val Loss 0.0279\n","iter 45: loss 0.0113, time 12044.89ms, mfu 10.19%\n","iter 46: loss 0.0032, time 9496.59ms, mfu 10.29%\n","iter 47: loss 0.0063, time 9407.61ms, mfu 10.40%\n","iter 48: loss 0.0019, time 9107.33ms, mfu 10.53%\n","iter 49: loss 0.0012, time 9421.11ms, mfu 10.61%\n","Evaluation at Iter 50: Val Loss 0.0356\n","iter 50: loss 0.0056, time 12362.58ms, mfu 10.41%\n","iter 51: loss 0.0054, time 10093.47ms, mfu 10.42%\n","iter 52: loss 0.0076, time 9682.80ms, mfu 10.48%\n","iter 53: loss 0.0037, time 9694.43ms, mfu 10.53%\n","iter 54: loss 0.0092, time 9752.59ms, mfu 10.57%\n","Evaluation at Iter 55: Val Loss 0.0187\n","Best val loss so far, saving Best Val checkpoint...\n","iter 55: loss 0.0143, time 24294.22ms, mfu 9.95%\n","iter 56: loss 0.0481, time 9501.60ms, mfu 10.08%\n","iter 57: loss 0.0054, time 9688.85ms, mfu 10.17%\n","iter 58: loss 0.0705, time 9799.38ms, mfu 10.24%\n","iter 59: loss 0.0328, time 9367.66ms, mfu 10.36%\n","Evaluation at Iter 60: Val Loss 0.0187\n","Best val loss so far, saving Best Val checkpoint...\n","iter 60: loss 0.0444, time 22148.49ms, mfu 9.80%\n","iter 61: loss 0.0131, time 9450.48ms, mfu 9.95%\n","iter 62: loss 0.0327, time 9722.21ms, mfu 10.05%\n","iter 63: loss 0.0148, time 9724.87ms, mfu 10.14%\n","iter 64: loss 0.0030, time 9681.87ms, mfu 10.23%\n","Evaluation at Iter 65: Val Loss 0.0188\n","iter 65: loss 0.0031, time 12233.03ms, mfu 10.08%\n","iter 66: loss 0.0058, time 9458.32ms, mfu 10.20%\n","iter 67: loss 0.0271, time 9341.35ms, mfu 10.32%\n","iter 68: loss 0.0128, time 9470.92ms, mfu 10.41%\n","iter 69: loss 0.0499, time 9500.54ms, mfu 10.49%\n","Evaluation at Iter 70: Val Loss 0.0183\n","Best val loss so far, saving Best Val checkpoint...\n","iter 70: loss 0.0011, time 27137.83ms, mfu 9.84%\n","iter 71: loss 0.0460, time 9732.93ms, mfu 9.95%\n","iter 72: loss 0.0314, time 9626.10ms, mfu 10.06%\n","iter 73: loss 0.0029, time 9733.86ms, mfu 10.15%\n","iter 74: loss 0.0013, time 9696.63ms, mfu 10.24%\n","Evaluation at Iter 75: Val Loss 0.0357\n","iter 75: loss 0.0488, time 12546.83ms, mfu 10.06%\n","iter 76: loss 0.0022, time 9680.37ms, mfu 10.16%\n","iter 77: loss 0.0487, time 9088.24ms, mfu 10.31%\n","iter 78: loss 0.0191, time 9367.13ms, mfu 10.42%\n","iter 79: loss 0.0033, time 9398.79ms, mfu 10.51%\n","Evaluation at Iter 80: Val Loss 0.0309\n","iter 80: loss 0.0159, time 12418.56ms, mfu 10.32%\n","number of parameters: 372.65M\n","100% 1066/1066 [03:10<00:00,  5.61it/s]\n","Best Val Checkpoint || Accuracy: 0.8602251407129456, Positive Predictions: 516, Negative Predictions: 531, Correct Predictions: 917\n","number of parameters: 372.65M\n"," 91% 971/1066 [02:53<00:13,  7.15it/s]"]}],"source":["!python train.py --init_from=\"gpt2-medium\" --out_dir=\"gpt-default-lora\""]},{"cell_type":"markdown","metadata":{"id":"HK_G-XxqQw6N"},"source":["# LoRA with rank = 196, alpha = 784"]},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"1jwaxUhtQw6N","outputId":"6f5c61f2-d339-4233-fa7c-5ea600c30d96","executionInfo":{"status":"ok","timestamp":1742057205668,"user_tz":240,"elapsed":1431088,"user":{"displayName":"Xu Jessica","userId":"02326492230102720065"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["2025-03-15 16:20:56.814983: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1742055656.899725   18934 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1742055656.939422   18934 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2025-03-15 16:20:57.430213: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mxulixin\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.8\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/MyDrive/handout/wandb/run-20250315_162104-i36r8sj7\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mgpt-r196_a784-lora-r:196-alph:784-lr:0.00025-iter:80\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/xulixin/HW3_lora_finetune_handout\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/xulixin/HW3_lora_finetune_handout/runs/i36r8sj7\u001b[0m\n","ddp False\n","tokens per iteration will be: 131,072\n","Map: 100% 5000/5000 [00:02<00:00, 1778.80 examples/s]\n","Map: 100% 1066/1066 [00:00<00:00, 1891.40 examples/s]\n","Positive: 2478, Negative: 2522\n","loading weights from pretrained gpt: gpt2-medium\n","forcing vocab_size=50257, block_size=1024, bias=True\n","overriding dropout rate to 0.0\n","overriding lora_rank and lora_alpha to 196, 784\n","overriding lora_dropout to 0.05\n","number of parameters: 382.68M\n","/content/drive/MyDrive/handout/train.py:237: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler(enabled=(args.dtype == 'float16'))\n","num decayed parameter tensors: 96, with 28,901,376 parameters\n","num non-decayed parameter tensors: 0, with 0 parameters\n","using fused AdamW: True\n","iter 1: loss 9.4154, time 11960.41ms, mfu -100.00%\n","iter 2: loss 4.3577, time 10837.52ms, mfu -100.00%\n","iter 3: loss 0.6156, time 11052.74ms, mfu -100.00%\n","iter 4: loss 0.0449, time 11048.58ms, mfu -100.00%\n","Evaluation at Iter 5: Val Loss 0.1418\n","Best val loss so far, saving Best Val checkpoint...\n","iter 5: loss 0.2663, time 30135.04ms, mfu -100.00%\n","iter 6: loss 0.0238, time 11144.07ms, mfu 9.79%\n","iter 7: loss 0.1171, time 10946.27ms, mfu 9.81%\n","iter 8: loss 0.0320, time 11412.42ms, mfu 9.79%\n","iter 9: loss 0.0910, time 11089.73ms, mfu 9.79%\n","Evaluation at Iter 10: Val Loss 0.0455\n","Best val loss so far, saving Best Val checkpoint...\n","iter 10: loss 0.0788, time 26217.23ms, mfu 9.23%\n","iter 11: loss 0.1545, time 10906.50ms, mfu 9.31%\n","iter 12: loss 0.0412, time 11719.82ms, mfu 9.31%\n","iter 13: loss 0.0442, time 11531.94ms, mfu 9.32%\n","iter 14: loss 0.0706, time 11031.86ms, mfu 9.38%\n","Evaluation at Iter 15: Val Loss 0.0356\n","Best val loss so far, saving Best Val checkpoint...\n","iter 15: loss 0.0307, time 27052.47ms, mfu 8.85%\n","iter 16: loss 0.0231, time 11230.78ms, mfu 8.93%\n","iter 17: loss 0.0357, time 11667.64ms, mfu 8.98%\n","iter 18: loss 0.0130, time 11885.31ms, mfu 9.00%\n","iter 19: loss 0.0374, time 11248.89ms, mfu 9.07%\n","Evaluation at Iter 20: Val Loss 0.0299\n","Best val loss so far, saving Best Val checkpoint...\n","iter 20: loss 0.0457, time 22137.00ms, mfu 8.65%\n","iter 21: loss 0.0107, time 11450.72ms, mfu 8.74%\n","iter 22: loss 0.0077, time 11503.41ms, mfu 8.82%\n","iter 23: loss 0.0303, time 11394.28ms, mfu 8.89%\n","iter 24: loss 0.0327, time 11370.03ms, mfu 8.96%\n","Evaluation at Iter 25: Val Loss 0.0292\n","Best val loss so far, saving Best Val checkpoint...\n","iter 25: loss 0.0178, time 24722.57ms, mfu 8.51%\n","iter 26: loss 0.0030, time 11373.67ms, mfu 8.62%\n","iter 27: loss 0.0078, time 11597.37ms, mfu 8.70%\n","iter 28: loss 0.0164, time 11271.15ms, mfu 8.79%\n","iter 29: loss 0.0052, time 11190.60ms, mfu 8.89%\n","Evaluation at Iter 30: Val Loss 0.0208\n","Best val loss so far, saving Best Val checkpoint...\n","iter 30: loss 0.0245, time 28090.28ms, mfu 8.39%\n","iter 31: loss 0.0042, time 11244.72ms, mfu 8.52%\n","iter 32: loss 0.0261, time 11347.50ms, mfu 8.63%\n","iter 33: loss 0.0800, time 11425.85ms, mfu 8.72%\n","iter 34: loss 0.0052, time 11174.82ms, mfu 8.83%\n","Evaluation at Iter 35: Val Loss 0.0443\n","iter 35: loss 0.0058, time 14282.08ms, mfu 8.71%\n","iter 36: loss 0.0035, time 11188.89ms, mfu 8.81%\n","iter 37: loss 0.0718, time 10959.34ms, mfu 8.93%\n","iter 38: loss 0.0057, time 11030.84ms, mfu 9.03%\n","iter 39: loss 0.0487, time 10970.80ms, mfu 9.12%\n","Evaluation at Iter 40: Val Loss 0.0240\n","iter 40: loss 0.0155, time 14361.31ms, mfu 8.97%\n","iter 41: loss 0.0029, time 11212.74ms, mfu 9.04%\n","iter 42: loss 0.0426, time 11309.11ms, mfu 9.10%\n","iter 43: loss 0.0108, time 11366.20ms, mfu 9.15%\n","iter 44: loss 0.0361, time 10916.43ms, mfu 9.24%\n","Evaluation at Iter 45: Val Loss 0.0139\n","Best val loss so far, saving Best Val checkpoint...\n","iter 45: loss 0.0009, time 25792.18ms, mfu 8.74%\n","iter 46: loss 0.0047, time 11643.24ms, mfu 8.80%\n","iter 47: loss 0.0303, time 11230.80ms, mfu 8.89%\n","iter 48: loss 0.0330, time 11554.05ms, mfu 8.95%\n","iter 49: loss 0.0793, time 11175.74ms, mfu 9.03%\n","Evaluation at Iter 50: Val Loss 0.0352\n","iter 50: loss 0.0016, time 14011.99ms, mfu 8.91%\n","iter 51: loss 0.0006, time 11057.86ms, mfu 9.00%\n","iter 52: loss 0.0030, time 10980.77ms, mfu 9.10%\n","iter 53: loss 0.0175, time 10984.48ms, mfu 9.18%\n","iter 54: loss 0.0892, time 10955.76ms, mfu 9.26%\n","Evaluation at Iter 55: Val Loss 0.0266\n","iter 55: loss 0.0041, time 14382.70ms, mfu 9.09%\n","iter 56: loss 0.0247, time 11276.78ms, mfu 9.15%\n","iter 57: loss 0.0086, time 10875.64ms, mfu 9.24%\n","iter 58: loss 0.0037, time 11326.61ms, mfu 9.28%\n","iter 59: loss 0.0033, time 11150.97ms, mfu 9.33%\n","Evaluation at Iter 60: Val Loss 0.0221\n","iter 60: loss 0.0487, time 14467.00ms, mfu 9.15%\n","iter 61: loss 0.0211, time 11180.44ms, mfu 9.21%\n","iter 62: loss 0.0156, time 11161.98ms, mfu 9.27%\n","iter 63: loss 0.0077, time 11144.18ms, mfu 9.32%\n","iter 64: loss 0.0184, time 11232.41ms, mfu 9.36%\n","Evaluation at Iter 65: Val Loss 0.0220\n","iter 65: loss 0.0046, time 14478.88ms, mfu 9.18%\n","iter 66: loss 0.0114, time 11200.18ms, mfu 9.24%\n","iter 67: loss 0.0032, time 11161.13ms, mfu 9.29%\n","iter 68: loss 0.0114, time 11227.35ms, mfu 9.33%\n","iter 69: loss 0.0311, time 11213.89ms, mfu 9.37%\n","Evaluation at Iter 70: Val Loss 0.0453\n","iter 70: loss 0.0029, time 14160.42ms, mfu 9.21%\n","iter 71: loss 0.0516, time 11025.12ms, mfu 9.28%\n","iter 72: loss 0.0147, time 10756.02ms, mfu 9.36%\n","iter 73: loss 0.0054, time 11007.53ms, mfu 9.42%\n","iter 74: loss 0.0051, time 11101.98ms, mfu 9.46%\n","Evaluation at Iter 75: Val Loss 0.0116\n","Best val loss so far, saving Best Val checkpoint...\n","iter 75: loss 0.0010, time 22037.47ms, mfu 9.01%\n","iter 76: loss 0.0016, time 11092.09ms, mfu 9.09%\n","iter 77: loss 0.0034, time 11326.09ms, mfu 9.15%\n","iter 78: loss 0.0509, time 11454.87ms, mfu 9.18%\n","iter 79: loss 0.0039, time 11095.51ms, mfu 9.25%\n","Evaluation at Iter 80: Val Loss 0.0245\n","iter 80: loss 0.0524, time 14574.70ms, mfu 9.07%\n","number of parameters: 382.68M\n","100% 1066/1066 [03:18<00:00,  5.38it/s]\n","Best Val Checkpoint || Accuracy: 0.8724202626641651, Positive Predictions: 471, Negative Predictions: 595, Correct Predictions: 930\n","number of parameters: 382.68M\n","100% 1066/1066 [03:12<00:00,  5.55it/s]\n","Last Iter Checkpoint || Accuracy: 0.8883677298311444, Positive Predictions: 500, Negative Predictions: 566, Correct Predictions: 947\n","\u001b[1;34mwandb\u001b[0m: \n","\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mgpt-r196_a784-lora-r:196-alph:784-lr:0.00025-iter:80\u001b[0m at: \u001b[34mhttps://wandb.ai/xulixin/HW3_lora_finetune_handout/runs/i36r8sj7\u001b[0m\n","\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250315_162104-i36r8sj7/logs\u001b[0m\n"]}],"source":["!python train.py --init_from=\"gpt2-medium\" --out_dir=\"gpt-r196_a784-lora\" --lora_rank=196 --lora_alpha=784"]},{"cell_type":"markdown","metadata":{"id":"26QOyuEyQw6N"},"source":["# LoRA with rank = 16, alpha = 256"]},{"cell_type":"code","execution_count":8,"metadata":{"colab":{"base_uri":"https://localhost:8080/"},"id":"8O6dICFrQw6N","outputId":"043d4fb4-7da7-4d2f-9350-627a46cd4a4e","executionInfo":{"status":"ok","timestamp":1742085324293,"user_tz":240,"elapsed":2171123,"user":{"displayName":"Xu Jessica","userId":"02326492230102720065"}}},"outputs":[{"output_type":"stream","name":"stdout","text":["2025-03-15 23:59:19.562160: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1742083159.582731    7407 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1742083159.589308    7407 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2025-03-15 23:59:19.611978: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n","\u001b[34m\u001b[1mwandb\u001b[0m: (1) Create a W&B account\n","\u001b[34m\u001b[1mwandb\u001b[0m: (2) Use an existing W&B account\n","\u001b[34m\u001b[1mwandb\u001b[0m: (3) Don't visualize my results\n","\u001b[34m\u001b[1mwandb\u001b[0m: Enter your choice: 2\n","\u001b[34m\u001b[1mwandb\u001b[0m: You chose 'Use an existing W&B account'\n","\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n","\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit: \n","\u001b[34m\u001b[1mwandb\u001b[0m: No netrc file found, creating one.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mxulixin\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.8\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/MyDrive/handout/wandb/run-20250316_001150-lu3z08e2\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mgpt-r16_a256-lora-r:16-alph:256-lr:0.00025-iter:80\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/xulixin/HW3_lora_finetune_handout\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/xulixin/HW3_lora_finetune_handout/runs/lu3z08e2\u001b[0m\n","ddp False\n","tokens per iteration will be: 131,072\n","Generating train split: 100% 8530/8530 [00:00<00:00, 9521.98 examples/s]\n","Generating validation split: 100% 1066/1066 [00:00<00:00, 170673.29 examples/s]\n","Generating test split: 100% 1066/1066 [00:00<00:00, 123412.96 examples/s]\n","tokenizer_config.json: 100% 26.0/26.0 [00:00<00:00, 60.4kB/s]\n","config.json: 100% 665/665 [00:00<00:00, 1.34MB/s]\n","vocab.json: 100% 1.04M/1.04M [00:00<00:00, 1.59MB/s]\n","merges.txt: 100% 456k/456k [00:00<00:00, 2.10MB/s]\n","tokenizer.json: 100% 1.36M/1.36M [00:00<00:00, 41.1MB/s]\n","Map: 100% 5000/5000 [00:02<00:00, 2025.89 examples/s]\n","Map: 100% 1066/1066 [00:00<00:00, 2369.37 examples/s]\n","Positive: 2478, Negative: 2522\n","loading weights from pretrained gpt: gpt2-medium\n","forcing vocab_size=50257, block_size=1024, bias=True\n","overriding dropout rate to 0.0\n","overriding lora_rank and lora_alpha to 16, 256\n","overriding lora_dropout to 0.05\n","number of parameters: 356.13M\n","config.json: 100% 718/718 [00:00<00:00, 1.21MB/s]\n","model.safetensors: 100% 1.52G/1.52G [00:16<00:00, 90.8MB/s]\n","generation_config.json: 100% 124/124 [00:00<00:00, 307kB/s]\n","/content/drive/MyDrive/handout/train.py:237: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler(enabled=(args.dtype == 'float16'))\n","num decayed parameter tensors: 96, with 2,359,296 parameters\n","num non-decayed parameter tensors: 0, with 0 parameters\n","using fused AdamW: True\n","iter 1: loss 9.2426, time 12200.75ms, mfu -100.00%\n","iter 2: loss 5.9451, time 11145.42ms, mfu -100.00%\n","iter 3: loss 3.3372, time 11031.52ms, mfu -100.00%\n","iter 4: loss 1.5181, time 10650.37ms, mfu -100.00%\n","Evaluation at Iter 5: Val Loss 1.0043\n","Best val loss so far, saving Best Val checkpoint...\n","iter 5: loss 1.4733, time 19558.34ms, mfu -100.00%\n","iter 6: loss 0.1416, time 10905.82ms, mfu 9.39%\n","iter 7: loss 0.0441, time 11028.45ms, mfu 9.38%\n","iter 8: loss 0.0878, time 11278.24ms, mfu 9.35%\n","iter 9: loss 0.0436, time 10740.95ms, mfu 9.37%\n","Evaluation at Iter 10: Val Loss 0.0767\n","Best val loss so far, saving Best Val checkpoint...\n","iter 10: loss 0.0265, time 20266.98ms, mfu 8.94%\n","iter 11: loss 0.0200, time 10804.69ms, mfu 8.99%\n","iter 12: loss 0.0244, time 10574.40ms, mfu 9.06%\n","iter 13: loss 0.0125, time 10949.76ms, mfu 9.09%\n","iter 14: loss 0.0468, time 10677.66ms, mfu 9.14%\n","Evaluation at Iter 15: Val Loss 0.0401\n","Best val loss so far, saving Best Val checkpoint...\n","iter 15: loss 0.0053, time 21412.86ms, mfu 8.71%\n","iter 16: loss 0.0121, time 10576.95ms, mfu 8.81%\n","iter 17: loss 0.0115, time 10954.82ms, mfu 8.86%\n","iter 18: loss 0.0099, time 10913.61ms, mfu 8.91%\n","iter 19: loss 0.0084, time 10998.31ms, mfu 8.95%\n","Evaluation at Iter 20: Val Loss 0.0382\n","Best val loss so far, saving Best Val checkpoint...\n","iter 20: loss 0.0033, time 22177.25ms, mfu 8.52%\n","iter 21: loss 0.0088, time 10916.26ms, mfu 8.61%\n","iter 22: loss 0.0061, time 10980.59ms, mfu 8.68%\n","iter 23: loss 0.0192, time 10830.35ms, mfu 8.76%\n","iter 24: loss 0.0140, time 10526.00ms, mfu 8.85%\n","Evaluation at Iter 25: Val Loss 0.0173\n","Best val loss so far, saving Best Val checkpoint...\n","iter 25: loss 0.1567, time 21521.55ms, mfu 8.45%\n","iter 26: loss 0.0047, time 10773.08ms, mfu 8.55%\n","iter 27: loss 0.0132, time 10788.24ms, mfu 8.65%\n","iter 28: loss 0.0045, time 11181.63ms, mfu 8.70%\n","iter 29: loss 0.0065, time 10627.98ms, mfu 8.79%\n","Evaluation at Iter 30: Val Loss 0.0186\n","iter 30: loss 0.0158, time 13916.26ms, mfu 8.65%\n","iter 31: loss 0.0097, time 10774.91ms, mfu 8.74%\n","iter 32: loss 0.0380, time 10650.39ms, mfu 8.82%\n","iter 33: loss 0.0432, time 10776.26ms, mfu 8.89%\n","iter 34: loss 0.0675, time 10659.31ms, mfu 8.96%\n","Evaluation at Iter 35: Val Loss 0.0261\n","iter 35: loss 0.0526, time 13872.67ms, mfu 8.81%\n","iter 36: loss 0.0581, time 10711.36ms, mfu 8.88%\n","iter 37: loss 0.0036, time 10666.40ms, mfu 8.95%\n","iter 38: loss 0.0113, time 10974.16ms, mfu 8.99%\n","iter 39: loss 0.0096, time 10659.71ms, mfu 9.05%\n","Evaluation at Iter 40: Val Loss 0.0356\n","iter 40: loss 0.0070, time 13857.47ms, mfu 8.89%\n","iter 41: loss 0.0112, time 10803.01ms, mfu 8.95%\n","iter 42: loss 0.0059, time 10880.32ms, mfu 8.99%\n","iter 43: loss 0.0403, time 10785.44ms, mfu 9.05%\n","iter 44: loss 0.0019, time 10778.06ms, mfu 9.09%\n","Evaluation at Iter 45: Val Loss 0.0247\n","iter 45: loss 0.0013, time 14299.59ms, mfu 8.90%\n","iter 46: loss 0.0082, time 10617.59ms, mfu 8.97%\n","iter 47: loss 0.0023, time 10900.60ms, mfu 9.02%\n","iter 48: loss 0.0017, time 10451.54ms, mfu 9.09%\n","iter 49: loss 0.0148, time 10274.52ms, mfu 9.18%\n","Evaluation at Iter 50: Val Loss 0.0303\n","iter 50: loss 0.0021, time 13947.80ms, mfu 9.00%\n","iter 51: loss 0.0051, time 10822.53ms, mfu 9.05%\n","iter 52: loss 0.0168, time 10690.27ms, mfu 9.10%\n","iter 53: loss 0.0775, time 10914.51ms, mfu 9.13%\n","iter 54: loss 0.0182, time 10604.17ms, mfu 9.18%\n","Evaluation at Iter 55: Val Loss 0.0337\n","iter 55: loss 0.0088, time 14015.82ms, mfu 8.99%\n","iter 56: loss 0.0013, time 10894.81ms, mfu 9.04%\n","iter 57: loss 0.0694, time 10825.46ms, mfu 9.08%\n","iter 58: loss 0.0483, time 10599.07ms, mfu 9.14%\n","iter 59: loss 0.0028, time 10973.42ms, mfu 9.16%\n","Evaluation at Iter 60: Val Loss 0.0219\n","iter 60: loss 0.0070, time 13853.86ms, mfu 8.98%\n","iter 61: loss 0.0383, time 10821.49ms, mfu 9.03%\n","iter 62: loss 0.0057, time 10500.72ms, mfu 9.10%\n","iter 63: loss 0.0035, time 10719.88ms, mfu 9.15%\n","iter 64: loss 0.0444, time 10595.85ms, mfu 9.20%\n","Evaluation at Iter 65: Val Loss 0.0191\n","iter 65: loss 0.0022, time 13804.05ms, mfu 9.02%\n","iter 66: loss 0.0010, time 10586.45ms, mfu 9.09%\n","iter 67: loss 0.0017, time 10763.38ms, mfu 9.13%\n","iter 68: loss 0.0194, time 10754.98ms, mfu 9.17%\n","iter 69: loss 0.0146, time 10647.75ms, mfu 9.22%\n","Evaluation at Iter 70: Val Loss 0.0150\n","Best val loss so far, saving Best Val checkpoint...\n","iter 70: loss 0.0047, time 20309.11ms, mfu 8.80%\n","iter 71: loss 0.0009, time 10989.53ms, mfu 8.85%\n","iter 72: loss 0.0979, time 10847.91ms, mfu 8.91%\n","iter 73: loss 0.0257, time 11091.67ms, mfu 8.94%\n","iter 74: loss 0.0037, time 10742.58ms, mfu 9.00%\n","Evaluation at Iter 75: Val Loss 0.0336\n","iter 75: loss 0.0151, time 13594.39ms, mfu 8.86%\n","iter 76: loss 0.0451, time 10797.22ms, mfu 8.92%\n","iter 77: loss 0.0080, time 10594.00ms, mfu 8.99%\n","iter 78: loss 0.0015, time 10618.83ms, mfu 9.06%\n","iter 79: loss 0.0272, time 10761.68ms, mfu 9.11%\n","Evaluation at Iter 80: Val Loss 0.0184\n","iter 80: loss 0.0107, time 13694.58ms, mfu 8.94%\n","number of parameters: 356.13M\n","100% 1066/1066 [02:51<00:00,  6.22it/s]\n","Best Val Checkpoint || Accuracy: 0.8667917448405253, Positive Predictions: 538, Negative Predictions: 524, Correct Predictions: 924\n","number of parameters: 356.13M\n","100% 1066/1066 [02:48<00:00,  6.34it/s]\n","Last Iter Checkpoint || Accuracy: 0.6651031894934334, Positive Predictions: 356, Negative Predictions: 449, Correct Predictions: 709\n","\u001b[1;34mwandb\u001b[0m: \n","\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mgpt-r16_a256-lora-r:16-alph:256-lr:0.00025-iter:80\u001b[0m at: \u001b[34mhttps://wandb.ai/xulixin/HW3_lora_finetune_handout/runs/lu3z08e2\u001b[0m\n","\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250316_001150-lu3z08e2/logs\u001b[0m\n"]}],"source":["!python train.py --init_from=\"gpt2-medium\" --out_dir=\"gpt-r16_a256-lora\" --lora_rank=16 --lora_alpha=256"]},{"cell_type":"markdown","metadata":{"id":"SYznD50eQw6N"},"source":["# Full Finetuning with Dropout of 0.05"]},{"cell_type":"code","execution_count":9,"metadata":{"id":"pfNWL6NkQw6N","colab":{"base_uri":"https://localhost:8080/"},"executionInfo":{"status":"ok","timestamp":1742087326730,"user_tz":240,"elapsed":2002100,"user":{"displayName":"Xu Jessica","userId":"02326492230102720065"}},"outputId":"4b6638f0-b9aa-46ae-b926-5f150d64f4ab"},"outputs":[{"output_type":"stream","name":"stdout","text":["2025-03-16 00:35:31.573237: E external/local_xla/xla/stream_executor/cuda/cuda_fft.cc:477] Unable to register cuFFT factory: Attempting to register factory for plugin cuFFT when one has already been registered\n","WARNING: All log messages before absl::InitializeLog() is called are written to STDERR\n","E0000 00:00:1742085331.594663   16386 cuda_dnn.cc:8310] Unable to register cuDNN factory: Attempting to register factory for plugin cuDNN when one has already been registered\n","E0000 00:00:1742085331.601308   16386 cuda_blas.cc:1418] Unable to register cuBLAS factory: Attempting to register factory for plugin cuBLAS when one has already been registered\n","2025-03-16 00:35:31.624003: I tensorflow/core/platform/cpu_feature_guard.cc:210] This TensorFlow binary is optimized to use available CPU instructions in performance-critical operations.\n","To enable the following instructions: AVX2 FMA, in other operations, rebuild TensorFlow with the appropriate compiler flags.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mxulixin\u001b[0m to \u001b[32mhttps://api.wandb.ai\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n","\u001b[34m\u001b[1mwandb\u001b[0m: Tracking run with wandb version 0.19.8\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run data is saved locally in \u001b[35m\u001b[1m/content/drive/MyDrive/handout/wandb/run-20250316_003535-2p0qvdp3\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: Run \u001b[1m`wandb offline`\u001b[0m to turn off syncing.\n","\u001b[34m\u001b[1mwandb\u001b[0m: Syncing run \u001b[33mgpt-r0-lora-d.05-r:0-alph:512-lr:0.00025-iter:80\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: ⭐️ View project at \u001b[34m\u001b[4mhttps://wandb.ai/xulixin/HW3_lora_finetune_handout\u001b[0m\n","\u001b[34m\u001b[1mwandb\u001b[0m: 🚀 View run at \u001b[34m\u001b[4mhttps://wandb.ai/xulixin/HW3_lora_finetune_handout/runs/2p0qvdp3\u001b[0m\n","ddp False\n","tokens per iteration will be: 131,072\n","Map: 100% 5000/5000 [00:02<00:00, 2041.79 examples/s]\n","Map: 100% 1066/1066 [00:00<00:00, 2314.48 examples/s]\n","Positive: 2478, Negative: 2522\n","loading weights from pretrained gpt: gpt2-medium\n","forcing vocab_size=50257, block_size=1024, bias=True\n","overriding dropout rate to 0.05\n","number of parameters: 353.77M\n","/content/drive/MyDrive/handout/train.py:237: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n","  scaler = torch.cuda.amp.GradScaler(enabled=(args.dtype == 'float16'))\n","num decayed parameter tensors: 98, with 354,501,632 parameters\n","num non-decayed parameter tensors: 194, with 321,536 parameters\n","using fused AdamW: True\n","iter 1: loss 8.2508, time 14824.25ms, mfu -100.00%\n","iter 2: loss 1.6120, time 14293.66ms, mfu -100.00%\n","iter 3: loss 0.2738, time 14323.05ms, mfu -100.00%\n","iter 4: loss 0.4530, time 14028.57ms, mfu -100.00%\n","Evaluation at Iter 5: Val Loss 0.0580\n","Best val loss so far, saving Best Val checkpoint...\n","iter 5: loss 0.1098, time 35239.15ms, mfu -100.00%\n","iter 6: loss 0.3036, time 14454.76ms, mfu 7.05%\n","iter 7: loss 0.0402, time 14788.54ms, mfu 7.03%\n","iter 8: loss 0.1120, time 14204.97ms, mfu 7.04%\n","iter 9: loss 0.0344, time 14193.19ms, mfu 7.06%\n","Evaluation at Iter 10: Val Loss 0.0553\n","Best val loss so far, saving Best Val checkpoint...\n","iter 10: loss 0.0629, time 37602.17ms, mfu 6.62%\n","iter 11: loss 0.0369, time 13928.74ms, mfu 6.69%\n","iter 12: loss 0.1081, time 14363.04ms, mfu 6.73%\n","iter 13: loss 0.0728, time 14394.63ms, mfu 6.77%\n","iter 14: loss 0.0871, time 14055.50ms, mfu 6.81%\n","Evaluation at Iter 15: Val Loss 0.0551\n","Best val loss so far, saving Best Val checkpoint...\n","iter 15: loss 0.0341, time 89555.59ms, mfu 6.25%\n","iter 16: loss 0.0673, time 14285.78ms, mfu 6.34%\n","iter 17: loss 0.0438, time 14329.11ms, mfu 6.41%\n","iter 18: loss 0.0566, time 14175.69ms, mfu 6.49%\n","iter 19: loss 0.0270, time 13859.11ms, mfu 6.58%\n","Evaluation at Iter 20: Val Loss 0.0510\n","Best val loss so far, saving Best Val checkpoint...\n","iter 20: loss 0.0338, time 48692.72ms, mfu 6.13%\n","iter 21: loss 0.0636, time 13700.75ms, mfu 6.26%\n","iter 22: loss 0.1164, time 14105.43ms, mfu 6.35%\n","iter 23: loss 0.0282, time 14250.80ms, mfu 6.43%\n","iter 24: loss 0.0296, time 13852.55ms, mfu 6.53%\n","Evaluation at Iter 25: Val Loss 0.0424\n","Best val loss so far, saving Best Val checkpoint...\n","iter 25: loss 0.0954, time 93474.23ms, mfu 5.98%\n","iter 26: loss 0.0626, time 14425.60ms, mfu 6.09%\n","iter 27: loss 0.0290, time 14126.75ms, mfu 6.20%\n","iter 28: loss 0.0458, time 14038.42ms, mfu 6.31%\n","iter 29: loss 0.0056, time 13834.96ms, mfu 6.41%\n","Evaluation at Iter 30: Val Loss 0.0540\n","iter 30: loss 0.0163, time 17167.50ms, mfu 6.36%\n","iter 31: loss 0.0253, time 14018.77ms, mfu 6.46%\n","iter 32: loss 0.0454, time 13958.85ms, mfu 6.54%\n","iter 33: loss 0.0332, time 14284.49ms, mfu 6.60%\n","iter 34: loss 0.0241, time 14118.43ms, mfu 6.66%\n","Evaluation at Iter 35: Val Loss 0.0428\n","iter 35: loss 0.0118, time 17247.47ms, mfu 6.58%\n","iter 36: loss 0.0279, time 13772.97ms, mfu 6.67%\n","iter 37: loss 0.0336, time 14046.46ms, mfu 6.72%\n","iter 38: loss 0.0654, time 13935.77ms, mfu 6.78%\n","iter 39: loss 0.0250, time 14374.76ms, mfu 6.81%\n","Evaluation at Iter 40: Val Loss 0.0319\n","Best val loss so far, saving Best Val checkpoint...\n","iter 40: loss 0.0037, time 107330.92ms, mfu 6.23%\n","iter 41: loss 0.0173, time 14613.02ms, mfu 6.30%\n","iter 42: loss 0.0464, time 14384.19ms, mfu 6.38%\n","iter 43: loss 0.0715, time 13787.58ms, mfu 6.48%\n","iter 44: loss 0.0289, time 14053.17ms, mfu 6.56%\n","Evaluation at Iter 45: Val Loss 0.0322\n","iter 45: loss 0.0215, time 16648.92ms, mfu 6.51%\n","iter 46: loss 0.0212, time 14169.13ms, mfu 6.58%\n","iter 47: loss 0.0201, time 14187.12ms, mfu 6.64%\n","iter 48: loss 0.0114, time 14012.84ms, mfu 6.70%\n","iter 49: loss 0.0038, time 13674.28ms, mfu 6.78%\n","Evaluation at Iter 50: Val Loss 0.0498\n","iter 50: loss 0.0411, time 17033.78ms, mfu 6.70%\n","iter 51: loss 0.0266, time 13886.61ms, mfu 6.76%\n","iter 52: loss 0.0032, time 13810.41ms, mfu 6.82%\n","iter 53: loss 0.0536, time 13908.47ms, mfu 6.87%\n","iter 54: loss 0.1872, time 14021.00ms, mfu 6.91%\n","Evaluation at Iter 55: Val Loss 0.0272\n","Best val loss so far, saving Best Val checkpoint...\n","iter 55: loss 0.0036, time 97775.17ms, mfu 6.33%\n","iter 56: loss 0.0052, time 14220.55ms, mfu 6.41%\n","iter 57: loss 0.0031, time 14036.10ms, mfu 6.49%\n","iter 58: loss 0.0271, time 13827.69ms, mfu 6.58%\n","iter 59: loss 0.0289, time 13912.01ms, mfu 6.66%\n","Evaluation at Iter 60: Val Loss 0.0612\n","iter 60: loss 0.0062, time 16900.38ms, mfu 6.59%\n","iter 61: loss 0.0124, time 13951.88ms, mfu 6.66%\n","iter 62: loss 0.0572, time 14546.50ms, mfu 6.70%\n","iter 63: loss 0.1152, time 14289.17ms, mfu 6.74%\n","iter 64: loss 0.0541, time 14134.61ms, mfu 6.79%\n","Evaluation at Iter 65: Val Loss 0.0551\n","iter 65: loss 0.0307, time 17084.06ms, mfu 6.70%\n","iter 66: loss 0.0273, time 13928.90ms, mfu 6.77%\n","iter 67: loss 0.0057, time 13933.48ms, mfu 6.82%\n","iter 68: loss 0.0039, time 14397.11ms, mfu 6.85%\n","iter 69: loss 0.0078, time 14164.32ms, mfu 6.88%\n","Evaluation at Iter 70: Val Loss 0.0420\n","iter 70: loss 0.0202, time 17238.16ms, mfu 6.78%\n","iter 71: loss 0.0017, time 14301.99ms, mfu 6.82%\n","iter 72: loss 0.0053, time 14142.36ms, mfu 6.86%\n","iter 73: loss 0.1184, time 13892.21ms, mfu 6.90%\n","iter 74: loss 0.0299, time 13762.76ms, mfu 6.95%\n","Evaluation at Iter 75: Val Loss 0.0492\n","iter 75: loss 0.1014, time 17507.28ms, mfu 6.84%\n","iter 76: loss 0.0369, time 14053.53ms, mfu 6.88%\n","iter 77: loss 0.0046, time 13869.03ms, mfu 6.93%\n","iter 78: loss 0.0037, time 14065.58ms, mfu 6.96%\n","iter 79: loss 0.0033, time 13949.44ms, mfu 6.99%\n","Evaluation at Iter 80: Val Loss 0.0347\n","iter 80: loss 0.0496, time 17301.93ms, mfu 6.88%\n","number of parameters: 353.77M\n","100% 1066/1066 [02:42<00:00,  6.54it/s]\n","Best Val Checkpoint || Accuracy: 0.7457786116322702, Positive Predictions: 774, Negative Predictions: 292, Correct Predictions: 795\n","number of parameters: 353.77M\n","100% 1066/1066 [02:15<00:00,  7.88it/s]\n","Last Iter Checkpoint || Accuracy: 0.8227016885553471, Positive Predictions: 632, Negative Predictions: 434, Correct Predictions: 877\n","\u001b[1;34mwandb\u001b[0m: \n","\u001b[1;34mwandb\u001b[0m: 🚀 View run \u001b[33mgpt-r0-lora-d.05-r:0-alph:512-lr:0.00025-iter:80\u001b[0m at: \u001b[34mhttps://wandb.ai/xulixin/HW3_lora_finetune_handout/runs/2p0qvdp3\u001b[0m\n","\u001b[1;34mwandb\u001b[0m: Find logs at: \u001b[1;35mwandb/run-20250316_003535-2p0qvdp3/logs\u001b[0m\n"]}],"source":["!python train.py --init_from=\"gpt2-medium\" --out_dir=\"gpt-r0-lora-d.05\" --lora_rank=0 --dropout=.05"]}],"metadata":{"accelerator":"GPU","colab":{"gpuType":"T4","provenance":[]},"kernelspec":{"display_name":"Python 3","name":"python3"},"language_info":{"name":"python"}},"nbformat":4,"nbformat_minor":0}