{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# On Kaggle\n",
    "# 1. Attach a GPU (see recitation for details on how to do these steps.)\n",
    "# 2. Enable file persistence (see recitation for details on how to do these steps.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Uncomment this on Colab\n",
    "\n",
    "# # Mount your Google Drive to Colab\n",
    "# from google.colab import drive\n",
    "# drive.mount('/content/drive')\n",
    "# %cd path_to_your_project\n",
    "# !pwd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Fetch the homework files.\n",
    "# # On Colab, if you save to your drive (by running %cd path_to_your_project), you only need to do this once.\n",
    "# !wget http://www.cs.cmu.edu/~mgormley/courses/10423/homework/hw2.zip\n",
    "# !unzip hw2.zip\n",
    "# !mv handout/* ."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Set up the environment\n",
    "# !pip install --quiet -r requirements.txt\n",
    "\n",
    "from utils import train_diffusion, visualize_diffusion\n",
    "import os"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "## COPY / PASTE YOUR ENTIRE DIFFUSION.PY FILE HERE\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from tqdm import tqdm\n",
    "\n",
    "import wandb\n",
    "\n",
    "def extract(a, t, x_shape):\n",
    "    \"\"\"\n",
    "    This function abstracts away the tedious indexing that would otherwise have\n",
    "    to be done to properly compute the diffusion equations from lecture. This\n",
    "    is necessary because we train data in batches, while the math taught in\n",
    "    lecture only considers a single sample.\n",
    "    \n",
    "    To use this function, consider the example\n",
    "        alpha_t * x\n",
    "    To compute this in code, we would write\n",
    "        extract(alpha, t, x.shape) * x\n",
    "\n",
    "    Args:\n",
    "        a: 1D tensor containing the value at each time step.\n",
    "        t: 1D tensor containing a batch of time indices.\n",
    "        x_shape: The reference shape.\n",
    "    Returns:\n",
    "        The extracted tensor.\n",
    "    \"\"\"\n",
    "    b, *_ = t.shape\n",
    "    out = a.gather(-1, t)\n",
    "    return out.reshape(b, *((1,) * (len(x_shape) - 1)))\n",
    "\n",
    "def cosine_schedule(timesteps, s=0.008):\n",
    "    \"\"\"\n",
    "    Passes the input timesteps through the cosine schedule for the diffusion process\n",
    "    Args:\n",
    "        timesteps: 1D tensor containing a batch of time indices.\n",
    "        s: The strength of the schedule.\n",
    "    Returns:\n",
    "        1D tensor of the same shape as timesteps, with the computed alpha.\n",
    "    \"\"\"\n",
    "    steps = timesteps + 1\n",
    "    x = torch.linspace(0, steps, steps)\n",
    "    alpha_bar = torch.cos(((x / steps) + s) / (1 + s) * torch.pi * 0.5) ** 2\n",
    "    alpha_bar = alpha_bar / alpha_bar[0]\n",
    "    alphas = alpha_bar[1:] / alpha_bar[:-1]\n",
    "    return torch.clip(alphas, 0.001, 1)\n",
    "\n",
    "\n",
    "# normalization functions\n",
    "def unnormalize_to_zero_to_one(t):\n",
    "    return (t + 1) * 0.5\n",
    "\n",
    "\n",
    "# DDPM implementation\n",
    "class Diffusion(nn.Module):\n",
    "    def __init__(\n",
    "        self,\n",
    "        model,\n",
    "        *,\n",
    "        image_size,\n",
    "        channels=3,\n",
    "        timesteps=1000,\n",
    "    ):\n",
    "        super().__init__()\n",
    "        self.channels = channels\n",
    "        self.image_size = image_size\n",
    "        self.model = model\n",
    "        self.num_timesteps = int(timesteps)\n",
    "\n",
    "        \"\"\"\n",
    "        Initializes the diffusion process.\n",
    "            1. Setup the schedule for the diffusion process.\n",
    "            2. Define the coefficients for the diffusion process.\n",
    "        Args:\n",
    "            model: The model to use for the diffusion process.\n",
    "            image_size: The size of the images.\n",
    "            channels: The number of channels in the images.\n",
    "            timesteps: The number of timesteps for the diffusion process.\n",
    "        \"\"\"\n",
    "        ## TODO: Implement the initialization of the diffusion process ##\n",
    "        # 1. define the scheduler here\n",
    "        # 2. pre-compute the coefficients for the diffusion process\n",
    "        # ###########################################################\n",
    "\n",
    "        # per-step α values\n",
    "        T = timesteps\n",
    "        s = 0.008\n",
    "        alphas = cosine_schedule(timesteps=T, s=s)  # shape: [T], 0 to T-1\n",
    "        self.register_buffer('alphas', alphas)\n",
    "        betas = 1 - alphas\n",
    "        self.register_buffer('betas', betas)\n",
    "\n",
    "        # cumulative product of α values for use in q_sample\n",
    "        # steps = T + 1  # t = 0,...,T\n",
    "        # x = torch.linspace(0, steps, steps)\n",
    "        # alpha_bar_full = torch.cos(((x / steps) + s) / (1 + s) * torch.pi * 0.5) ** 2  # in new paper\n",
    "        # alpha_bar_full = alpha_bar_full / alpha_bar_full[0]  # shape: [T+1], from -1, with 1 at [0]\n",
    "        # alpha_bar = alpha_bar_full[1:]  # shape: [T]. For t = 1,...,T, product of all previous α up to t\n",
    "        alpha_bar = torch.cumprod(alphas, dim=0)\n",
    "        alpha_bar_prev = F.pad(alpha_bar[:-1], (1, 0), value=1.0)\n",
    "        self.register_buffer('alpha_bar', alpha_bar)\n",
    "        self.register_buffer('alpha_bar_prev', alpha_bar_prev)\n",
    "        self.register_buffer('sqrt_alpha_bar', torch.sqrt(alpha_bar))\n",
    "        self.register_buffer('sqrt_one_minus_alpha_bar', torch.sqrt(1 - alpha_bar))\n",
    "\n",
    "        # Compute posterior variance for the reverse process:\n",
    "        # For t >= 1, σₜ² = (1 - ᾱ₍ₜ₋₁₎) / (1 - ᾱₜ) * βₜ, with convention ᾱ₋₁ = 1.\n",
    "        # q_var = torch.empty_like(betas)  # shape: [T]\n",
    "        q_var = betas * (1 - alpha_bar_prev) / (1 - alpha_bar)\n",
    "        # q_var[0] = 0.0  # at t==0\n",
    "        # for t in range(1, T):\n",
    "        #     # Note: alpha_bar_full[t] corresponds to ᾱₜ₋₁ and alpha_bar_full[t+1] to ᾱₜ.\n",
    "        #     # q_var[t] = betas[t] * (1 - alpha_bar_full[t]) / (1 - alpha_bar_full[t+1])\n",
    "        #     q_var[t] = betas[t] * (1 - alpha_bar[t-1]) / (1 - alpha_bar[t])\n",
    "        self.register_buffer('q_var', q_var)\n",
    "\n",
    "    def noise_like(self, shape, device):\n",
    "        \"\"\"\n",
    "        Generates noise with the same shape as the input.\n",
    "        Args:\n",
    "            shape: The shape of the noise.\n",
    "            device: The device on which to create the noise.\n",
    "        Returns:\n",
    "            The generated noise.\n",
    "        \"\"\"\n",
    "        noise = lambda: torch.randn(shape, device=device)\n",
    "        return noise()\n",
    "\n",
    "    # backward diffusion\n",
    "    @torch.no_grad()\n",
    "    def p_sample(self, x, t, t_index):\n",
    "        \"\"\"\n",
    "        Computes the (t_index)th sample from the (t_index + 1)th sample using\n",
    "        the reverse diffusion process.\n",
    "        Args:\n",
    "            x: The sampled image at timestep t_index + 1.\n",
    "            t: 1D tensor of the index of the time step.\n",
    "            t_index: Scalar of the index of the time step.\n",
    "        Returns:\n",
    "            The sampled image at timestep t_index.\n",
    "        \"\"\"\n",
    "        ####### TODO: Implement the p_sample function #######\n",
    "        # sample x_{t-1} from the gaussian distribution wrt. posterior mean and posterior variance\n",
    "        # Hint: use extract function to get the coefficients at time t\n",
    "        # Hint: use self.noise_like function to generate noise. DO NOT USE torch.randn\n",
    "        # Begin code here#######################################################################\n",
    "\n",
    "        # Predict noise using the model, i.e. ϵθ(xₜ, t)\n",
    "        eps_theta = self.model(x, t)\n",
    "        # Estimate x₀: x₀_pred = (x - √(1 - ᾱₜ) * ϵθ) / √(ᾱₜ)\n",
    "        sqrt_alpha_bar = extract(self.sqrt_alpha_bar, t, x.shape)\n",
    "        sqrt_one_minus_alpha_bar = extract(self.sqrt_one_minus_alpha_bar, t, x.shape)\n",
    "        x0_pred = (x - sqrt_one_minus_alpha_bar * eps_theta) / sqrt_alpha_bar\n",
    "        x0_pred = torch.clamp(x0_pred, -1, 1)\n",
    "\n",
    "        # Extract coefficients needed for the reverse process.\n",
    "        beta_t = extract(self.betas, t, x.shape)\n",
    "        alpha_t = extract(self.alphas, t, x.shape)\n",
    "        alpha_bar_t = extract(self.alpha_bar, t, x.shape)\n",
    "        # For t_index == 0, we set ᾱ₍ₜ₋₁₎ = 1.\n",
    "        # if t_index == 0:\n",
    "        #     alpha_bar_prev = torch.ones_like(alpha_bar_t)\n",
    "        # else:\n",
    "        #     alpha_bar_prev = extract(self.alpha_bar, t - 1, x.shape)\n",
    "\n",
    "        # Compute posterior mean:\n",
    "        # μ̃ₜ = √(αₜ) * ((1 - ᾱ₍ₜ₋₁₎) / (1 - ᾱₜ)) * xₜ +\n",
    "        #       √(ᾱ₍ₜ₋₁₎) * ((1 - αₜ) / (1 - ᾱₜ)) * x₀_pred\n",
    "        alpha_bar_prev = extract(self.alpha_bar_prev, t, x.shape)\n",
    "        posterior_mean = (torch.sqrt(alpha_t) * (1 - alpha_bar_prev) / (1 - alpha_bar_t)) * x + \\\n",
    "                         (torch.sqrt(alpha_bar_prev) * (1 - alpha_t) / (1 - alpha_bar_t)) * x0_pred\n",
    "\n",
    "        # Get posterior variance.\n",
    "        q_var = extract(self.q_var, t, x.shape)\n",
    "\n",
    "        # Add noise if not at the final step.\n",
    "        if t_index == 0:\n",
    "            noise = torch.zeros_like(x)\n",
    "        else:\n",
    "            noise = self.noise_like(x.shape, x.device)\n",
    "\n",
    "        return posterior_mean + torch.sqrt(q_var) * noise\n",
    "        # if t_index == 0:\n",
    "        #     return ...\n",
    "        # else:\n",
    "        #     return ...\n",
    "        # ####################################################\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def p_sample_loop(self, img):\n",
    "        \"\"\"\n",
    "        Passes noise through the entire reverse diffusion process to generate\n",
    "        final image samples.\n",
    "        Args:\n",
    "            img: The initial noise that is randomly sampled from the noise distribution.\n",
    "        Returns:\n",
    "            The sampled images.\n",
    "        \"\"\"\n",
    "        b = img.shape[0]\n",
    "        #### TODO: Implement the p_sample_loop function ####\n",
    "        # 1. loop through the time steps from the last to the first\n",
    "        # 2. inside the loop, sample x_{t-1} from the reverse diffusion process\n",
    "        # 3. clamp and unnormalize the generated image to valid pixel range\n",
    "        # Hint: to get time index, you can use torch.full()\n",
    "        # for i in reversed(range(self.num_timesteps)):\n",
    "        for i in range(self.num_timesteps - 1, -1, -1):\n",
    "            t = torch.full((b,), i, device=img.device, dtype=torch.long)\n",
    "            img = self.p_sample(img, t, i)\n",
    "        img = torch.clamp(img, -1, 1)\n",
    "        img = unnormalize_to_zero_to_one(img)\n",
    "        return img\n",
    "        # ####################################################\n",
    "\n",
    "    @torch.no_grad()\n",
    "    def sample(self, batch_size):\n",
    "        \"\"\"\n",
    "        Wrapper function for p_sample_loop.\n",
    "        Args:\n",
    "            batch_size: The number of images to sample.\n",
    "        Returns:\n",
    "            The sampled images.\n",
    "        \"\"\"\n",
    "        self.model.eval()\n",
    "        #### TODO: Implement the sample function ####\n",
    "        # Hint: use self.noise_like function to generate noise. DO NOT USE torch.randn\n",
    "        device = next(self.model.parameters()).device\n",
    "        img = self.noise_like((batch_size, self.channels, self.image_size, self.image_size), device)\n",
    "        img = self.p_sample_loop(img)\n",
    "        # img = ...\n",
    "        return img\n",
    "\n",
    "    # forward diffusion\n",
    "    def q_sample(self, x_0, t, noise):\n",
    "        \"\"\"\n",
    "        Applies alpha interpolation between x_0 and noise to simulate sampling\n",
    "        x_t from the noise distribution.\n",
    "        Args:\n",
    "            x_0: The initial images.\n",
    "            t: 1D tensor containing a batch of time indices to sample at.\n",
    "            noise: The noise tensor to sample from.\n",
    "        Returns:\n",
    "            The sampled images.\n",
    "        \"\"\"\n",
    "        ###### TODO: Implement the q_sample function #######\n",
    "        sqrt_alpha_bar = extract(self.sqrt_alpha_bar, t, x_0.shape)\n",
    "        sqrt_one_minus_alpha_bar = extract(self.sqrt_one_minus_alpha_bar, t, noise.shape)\n",
    "        x_t = sqrt_alpha_bar * x_0 + sqrt_one_minus_alpha_bar * noise\n",
    "        # x_t = ...\n",
    "        return x_t\n",
    "\n",
    "    def p_losses(self, x_0, t, noise):\n",
    "        \"\"\"\n",
    "        Computes the loss for the forward diffusion.\n",
    "        Args:\n",
    "            x_0: The initial images.\n",
    "            t: 1D tensor containing a batch of time indices to compute the loss at.\n",
    "            noise: The noise tensor to use.\n",
    "        Returns:\n",
    "            The computed loss.\n",
    "        \"\"\"\n",
    "        ###### TODO: Implement the p_losses function #######\n",
    "        # define loss function wrt. the model output and the target\n",
    "        # Hint: you can use pytorch built-in loss functions: F.l1_loss\n",
    "        x_t = self.q_sample(x_0, t, noise)\n",
    "        predicted_noise = self.model(x_t, t)\n",
    "        loss = F.l1_loss(predicted_noise, noise)\n",
    "        # loss = ...\n",
    "        return loss\n",
    "        # ####################################################\n",
    "\n",
    "    def forward(self, x_0, noise):\n",
    "        \"\"\"\n",
    "        Acts as a wrapper for p_losses.\n",
    "        Args:\n",
    "            x_0: The initial images.\n",
    "            noise: The noise tensor to use.\n",
    "        Returns:\n",
    "            The computed loss.\n",
    "        \"\"\"\n",
    "        b, c, h, w, device, img_size, = *x_0.shape, x_0.device, self.image_size\n",
    "        device = x_0.device\n",
    "        assert h == img_size and w == img_size, f'height and width of image must be {img_size}'\n",
    "        ###### TODO: Implement the forward function #######\n",
    "        t = torch.randint(0, self.num_timesteps, (b,), device=device).long()\n",
    "        return self.p_losses(x_0, t, noise)\n",
    "        # t = torch.randint(...)\n",
    "        # return ..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Q6.1\n",
    "# args = {\n",
    "#     \"save_folder\": \"./results/6_1/\",\n",
    "#     \"data_path\": \"./data/train\",\n",
    "#     \"train_steps\": 1000,\n",
    "#     \"save_and_sample_every\": 100,\n",
    "#     \"fid\": False,\n",
    "# }\n",
    "\n",
    "# Q6.2\n",
    "args = {\n",
    "    \"save_folder\": \"./results/6_2/\",\n",
    "    \"data_path\": \"./data/train\",\n",
    "    \"train_steps\": 1000,\n",
    "    \"save_and_sample_every\": 100,\n",
    "    \"fid\": True,\n",
    "}\n",
    "\n",
    "# # Q6.3\n",
    "# args = {\n",
    "#     \"save_folder\": \"./results/6_3\",\n",
    "#     \"data_path\": \"./data/train\",\n",
    "#     \"train_steps\": 10000,\n",
    "#     \"save_and_sample_every\": 1000,\n",
    "#     \"fid\": False,\n",
    "# }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mxulixin\u001b[0m. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
      "\u001b[34m\u001b[1mwandb\u001b[0m: Using wandb-core as the SDK backend.  Please refer to https://wandb.me/wandb-core for more information.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2c56ab38e724273b8b95bbbbf7b0612",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.011115914911109333, max=1.0…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/d/xulixin/CMU courses/10-623 Gen AI/HW2/handout/wandb/run-20250224_193913-pwacsxga</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/xulixin/DDPM_AFHQ/runs/pwacsxga' target=\"_blank\">summer-vortex-9</a></strong> to <a href='https://wandb.ai/xulixin/DDPM_AFHQ' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/xulixin/DDPM_AFHQ' target=\"_blank\">https://wandb.ai/xulixin/DDPM_AFHQ</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/xulixin/DDPM_AFHQ/runs/pwacsxga' target=\"_blank\">https://wandb.ai/xulixin/DDPM_AFHQ/runs/pwacsxga</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset length: 5153, dataset class: cat\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "steps:  10%|▉         | 99/1000 [00:37<05:13,  2.87it/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "compute FID between two folders\n",
      "Found 512 images in the folder results/6_2/sample_ddpm_0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FID sample_ddpm_0 : 100%|██████████| 16/16 [00:08<00:00,  1.87it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 1500 images in the folder ./data/val\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "FID val : 100%|██████████| 47/47 [00:08<00:00,  5.57it/s]\n",
      "steps:  10%|▉         | 99/1000 [01:21<12:20,  1.22it/s]\n"
     ]
    },
    {
     "ename": "ValueError",
     "evalue": "Imaginary component 4.3329484766992505e+108",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[7], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Train the model: If you use colab T4 to train the model, the training process will probably take 2 hours.\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m \u001b[43mtrain_diffusion\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mDiffusion\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mDiffusion\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/mnt/d/xulixin/CMU courses/10-623 Gen AI/HW2/handout/utils.py:118\u001b[0m, in \u001b[0;36mtrain_diffusion\u001b[0;34m(train_steps, save_and_sample_every, fid, save_folder, data_path, load_path, unet_dim_mults, image_size, batch_size, data_class, time_steps, unet_dim, learning_rate, dataloader_workers, Diffusion)\u001b[0m\n\u001b[1;32m     83\u001b[0m wandb\u001b[38;5;241m.\u001b[39minit(\n\u001b[1;32m     84\u001b[0m     project\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mDDPM_AFHQ\u001b[39m\u001b[38;5;124m\"\u001b[39m,\n\u001b[1;32m     85\u001b[0m     config\u001b[38;5;241m=\u001b[39m{\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     99\u001b[0m     name\u001b[38;5;241m=\u001b[39msave_folder\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m/\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m],\n\u001b[1;32m    100\u001b[0m )\n\u001b[1;32m    102\u001b[0m _, _, trainer \u001b[38;5;241m=\u001b[39m setup_diffusion(train_steps\u001b[38;5;241m=\u001b[39mtrain_steps, \n\u001b[1;32m    103\u001b[0m                                 save_and_sample_every\u001b[38;5;241m=\u001b[39msave_and_sample_every, \n\u001b[1;32m    104\u001b[0m                                 fid\u001b[38;5;241m=\u001b[39mfid, \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    115\u001b[0m                                 dataloader_workers\u001b[38;5;241m=\u001b[39mdataloader_workers, \n\u001b[1;32m    116\u001b[0m                                 Diffusion\u001b[38;5;241m=\u001b[39mDiffusion)\n\u001b[0;32m--> 118\u001b[0m \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    119\u001b[0m wandb\u001b[38;5;241m.\u001b[39mfinish()\n",
      "File \u001b[0;32m/mnt/d/xulixin/CMU courses/10-623 Gen AI/HW2/handout/trainer.py:224\u001b[0m, in \u001b[0;36mTrainer.train\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    221\u001b[0m             img \u001b[38;5;241m=\u001b[39m img\u001b[38;5;241m.\u001b[39mresize((\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_size, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mimage_size))\n\u001b[1;32m    222\u001b[0m             img\u001b[38;5;241m.\u001b[39msave(os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mval_folder, img_name))\n\u001b[0;32m--> 224\u001b[0m     fid_score \u001b[38;5;241m=\u001b[39m \u001b[43mfid\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcompute_fid\u001b[49m\u001b[43m(\u001b[49m\u001b[43msave_folder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mval_folder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    225\u001b[0m     log_dir[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfid\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m fid_score  \u001b[38;5;66;03m# ______________________________________\u001b[39;00m\n\u001b[1;32m    226\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msave()\n",
      "File \u001b[0;32m~/ENTER/envs/linux/lib/python3.10/site-packages/cleanfid/fid.py:478\u001b[0m, in \u001b[0;36mcompute_fid\u001b[0;34m(fdir1, fdir2, gen, mode, model_name, num_workers, batch_size, device, dataset_name, dataset_res, dataset_split, num_gen, z_dim, custom_feat_extractor, verbose, custom_image_tranform, custom_fn_resize, use_dataparallel)\u001b[0m\n\u001b[1;32m    476\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m verbose:\n\u001b[1;32m    477\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mcompute FID between two folders\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m--> 478\u001b[0m     score \u001b[38;5;241m=\u001b[39m \u001b[43mcompare_folders\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfdir1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfdir2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeat_model\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    479\u001b[0m \u001b[43m        \u001b[49m\u001b[43mmode\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmode\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbatch_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbatch_size\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    480\u001b[0m \u001b[43m        \u001b[49m\u001b[43mnum_workers\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnum_workers\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdevice\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdevice\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    481\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_image_tranform\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_image_tranform\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    482\u001b[0m \u001b[43m        \u001b[49m\u001b[43mcustom_fn_resize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mcustom_fn_resize\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    483\u001b[0m \u001b[43m        \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    484\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m score\n\u001b[1;32m    486\u001b[0m \u001b[38;5;66;03m# compute fid of a folder\u001b[39;00m\n",
      "File \u001b[0;32m~/ENTER/envs/linux/lib/python3.10/site-packages/cleanfid/fid.py:285\u001b[0m, in \u001b[0;36mcompare_folders\u001b[0;34m(fdir1, fdir2, feat_model, mode, num_workers, batch_size, device, verbose, custom_image_tranform, custom_fn_resize)\u001b[0m\n\u001b[1;32m    283\u001b[0m mu2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmean(np_feats2, axis\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m    284\u001b[0m sigma2 \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mcov(np_feats2, rowvar\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mFalse\u001b[39;00m)\n\u001b[0;32m--> 285\u001b[0m fid \u001b[38;5;241m=\u001b[39m \u001b[43mfrechet_distance\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmu1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma1\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmu2\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43msigma2\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    286\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m fid\n",
      "File \u001b[0;32m~/ENTER/envs/linux/lib/python3.10/site-packages/cleanfid/fid.py:58\u001b[0m, in \u001b[0;36mfrechet_distance\u001b[0;34m(mu1, sigma1, mu2, sigma2, eps)\u001b[0m\n\u001b[1;32m     56\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m np\u001b[38;5;241m.\u001b[39mallclose(np\u001b[38;5;241m.\u001b[39mdiagonal(covmean)\u001b[38;5;241m.\u001b[39mimag, \u001b[38;5;241m0\u001b[39m, atol\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m1e-3\u001b[39m):\n\u001b[1;32m     57\u001b[0m         m \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mmax(np\u001b[38;5;241m.\u001b[39mabs(covmean\u001b[38;5;241m.\u001b[39mimag))\n\u001b[0;32m---> 58\u001b[0m         \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mImaginary component \u001b[39m\u001b[38;5;132;01m{}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;241m.\u001b[39mformat(m))\n\u001b[1;32m     59\u001b[0m     covmean \u001b[38;5;241m=\u001b[39m covmean\u001b[38;5;241m.\u001b[39mreal\n\u001b[1;32m     61\u001b[0m tr_covmean \u001b[38;5;241m=\u001b[39m np\u001b[38;5;241m.\u001b[39mtrace(covmean)\n",
      "\u001b[0;31mValueError\u001b[0m: Imaginary component 4.3329484766992505e+108"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Fatal error while uploading data. Some run data will not be synced, but it will still be written to disk. Use `wandb sync` at the end of the run to try uploading.\n"
     ]
    }
   ],
   "source": [
    "# Train the model: If you use colab T4 to train the model, the training process will probably take 2 hours.\n",
    "train_diffusion(**args, Diffusion=Diffusion)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "Tracking run with wandb version 0.19.4"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Run data is saved locally in <code>/mnt/d/xulixin/CMU courses/10-623 Gen AI/HW2/handout/wandb/run-20250223_225842-kg3yc61x</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Syncing run <strong><a href='https://wandb.ai/xulixin/DDPM_AFHQ/runs/kg3yc61x' target=\"_blank\">6_3</a></strong> to <a href='https://wandb.ai/xulixin/DDPM_AFHQ' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/developer-guide' target=\"_blank\">docs</a>)<br>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View project at <a href='https://wandb.ai/xulixin/DDPM_AFHQ' target=\"_blank\">https://wandb.ai/xulixin/DDPM_AFHQ</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run at <a href='https://wandb.ai/xulixin/DDPM_AFHQ/runs/kg3yc61x' target=\"_blank\">https://wandb.ai/xulixin/DDPM_AFHQ/runs/kg3yc61x</a>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "dataset length: 5153, dataset class: cat\n",
      "Loading :  ./results/6_3/model.pt\n"
     ]
    },
    {
     "data": {
      "text/html": [],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       " View run <strong style=\"color:#cdcd00\">6_3</strong> at: <a href='https://wandb.ai/xulixin/DDPM_AFHQ/runs/kg3yc61x' target=\"_blank\">https://wandb.ai/xulixin/DDPM_AFHQ/runs/kg3yc61x</a><br> View project at: <a href='https://wandb.ai/xulixin/DDPM_AFHQ' target=\"_blank\">https://wandb.ai/xulixin/DDPM_AFHQ</a><br>Synced 5 W&B file(s), 10 media file(s), 0 artifact file(s) and 0 other file(s)"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "Find logs at: <code>./wandb/run-20250223_225842-kg3yc61x/logs</code>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Visualize the forward and backward process\n",
    "# \"./results/6_1/model.pt\"\n",
    "visualize_diffusion(**args, load_path=os.path.join(args[\"save_folder\"], 'model.pt'), Diffusion=Diffusion)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "linux",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
